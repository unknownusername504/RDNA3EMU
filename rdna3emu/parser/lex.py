import ply.lex as lex
import os

reserved = {
    ".set": "SET",
    ".globl": "GLOBAL",
    ".rodata": "KERNEL_DESC",
    ".data": "DATA",
    ".bss": "BSS",
    "SHN_AMDGPU_LDS": "LDS_GLOBAL",
    "vcc_lo": "VCC_LO",
    "vcc_hi": "VCC_HI",
    "m0": "M0",
}
# List of token names
tokens = (
    "TEXT",
    "TARGET",
    "WEAK",
    "P2ALIGN",
    "TYPE",
    "DIRECTIVE",
    "INTEGER",
    "HEX",
    "OCTAL",
    "BINARY",
    "FLOATING",
    "VGPR",
    "SGPR",
    "SYMBOL",
    "LABEL",
    "INSTRUCTION",
    "COMMA",
    "COLON",
    "COMMENT",
    "STRING",
    "LPAREN",
    "RPAREN",
    "PLUS",
    "MINUS",
    "AT",
    "OR",
    "HASH",
    "REF",
) + tuple(reserved.values())

# Regular expression rules for simple tokens
decimal = r"[-]?[1-9][0-9]*|0"
hex = r"[-]?0x[0-9a-fA-F]+ | [-]?[0x]?[0-9][0-9a-fA-F]*[hH]"
octal = r"[-]?0[0-7]+"
binary = r"[-]?0b[01]+"
integer = hex + r"|" + octal + r"|" + binary + r"|" + decimal
t_INTEGER = integer
t_FLOATING = r"[-]?[0-9]*[.][0-9]+([eE][+-]?[0-9]*)? | [-]0x[0-9a-fA-F]*(.[0-9a-fA-F]+)?[pP][+-]?[0-9a-fA-F]+"
t_COLON = r":"
t_LPAREN = r"\("
t_RPAREN = r"\)"
t_PLUS = r"\+"
t_MINUS = r"-"
t_AT = r"@"
t_OR = r"\|"
t_HASH = r"\#"


def t_REF(t):
    r"<[a-zA-Z_.][a-zA-Z0-9_$.@]*>"
    return t


def t_TEXT(t):
    r"\.text"
    return t


def t_TARGET(t):
    r"\.amdgcn_target"
    return t


def t_WEAK(t):
    r"\.weak"
    return t


def t_P2ALIGN(t):
    r"\.p2align"
    return t


def t_TYPE(t):
    r"\.type"
    return t


def t_COMMA(t):
    r","
    return t


def t_DIRECTIVE(t):
    r"\.[a-zA-Z_]+"
    return t


def t_VGPR(t):
    r"v\d+|v\[\d+:\d+\]|v\[\d+\]"
    return t


def t_SGPR(t):
    r"s\d+|s\[\d+:\d+\]|s\[\d+\]"
    return t


def t_INSTRUCTION(t):
    r"s_[a-zA-Z_\d+]+|v_[a-zA-Z_\d+]+"
    return t


def t_STRING(t):
    r'\"[^"]*\"'
    return t


# Define a rule so we can track line numbers
def t_newline(t):
    r"\n+"
    t.lexer.lineno += len(t.value)


def t_LABEL(t):
    r"[a-zA-Z_.][a-zA-Z0-9_$.@]*:"
    return t


def t_SYMBOL(t):
    r"[a-zA-Z_.][a-zA-Z0-9_$.@]*"
    t.type = reserved.get(t.value, "SYMBOL")
    return t


# A string containing ignored characters (spaces and tabs)
t_ignore = " \t"


# A rule to handle comments
def t_COMMENT(t):
    r"\/\/.*"
    pass  # Token discarded


# Error handling rule
def t_error(t):
    print(f"Illegal character '{t.value[0]}'")
    t.lexer.skip(1)


class Lexer:
    def __init__(self, dump_file_path_from_wa="Data/tinyconvdump.txt"):
        # Build the lexer
        self.lexer = lex.lex()
        self.dump_file_path = os.path.join(os.getcwd(), dump_file_path_from_wa)
        # Test it out with the file contents
        self.lexer.input(self.get_asm_code())

    # Interpret the RDNA3 ISA assembly code generated by the compiler (HIP)
    def get_asm_code(self):
        with open(self.dump_file_path, "r") as f:
            # Drop the function prologue which is 6 lines long
            for _ in range(6):
                next(f)
            data = f.read()
            return data

    def get_lex(self):
        return self.lexer

    def test(self):
        # Tokenize
        while True:
            tok = self.lexer.token()
            if not tok:
                break  # No more input
            print(tok)


def main():
    lexer = Lexer()
    lexer.test()


if __name__ == "__main__":
    main()
